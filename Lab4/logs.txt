* 
* ==> Audit <==
* |--------------|----------------------------------------------------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|   Command    |                                    Args                                    | Profile  |         User          | Version |     Start Time      |      End Time       |
|--------------|----------------------------------------------------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| kubectl      | -- expose pod                                                              | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 13:48 MSK |                     |
|              | webserver-866c8bfbd-929c4                                                  |          |                       |         |                     |                     |
|              | --type=NodePort --port=3000                                                |          |                       |         |                     |                     |
| kubectl      | -- expose pod webserver                                                    | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 13:50 MSK |                     |
|              | --type=NodePort --port=3000                                                |          |                       |         |                     |                     |
| kubectl      | -- expose pod                                                              | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 13:53 MSK |                     |
|              | webserver-26d93c580aed19d3cb3639c8118935936d73039f5be6f1e077e0bfaa46dd9892 |          |                       |         |                     |                     |
|              | --type=NodePort --port=3000                                                |          |                       |         |                     |                     |
| kubectl      | -- expose pod webserver                                                    | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:14 MSK |                     |
|              | --type=loadbalancer                                                        |          |                       |         |                     |                     |
|              | --port=3000                                                                |          |                       |         |                     |                     |
| kubectl      | -- expose pod webserver                                                    | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:15 MSK |                     |
|              | --type=LoadBalancer                                                        |          |                       |         |                     |                     |
|              | --port=3000                                                                |          |                       |         |                     |                     |
| kubectl      | -- expose deployment/webserver                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:23 MSK | 19 Nov 22 16:23 MSK |
|              | --type=NodePort --port=3000                                                |          |                       |         |                     |                     |
| kubectl      | -- port-forward service/vault                                              | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:25 MSK |                     |
|              | 8200:8200                                                                  |          |                       |         |                     |                     |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:27 MSK | 19 Nov 22 16:27 MSK |
| kubectl      | -- port-forward                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 19 Nov 22 16:30 MSK | 20 Nov 22 13:02 MSK |
|              | service/webserver 3000:3000                                                |          |                       |         |                     |                     |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 20 Nov 22 13:02 MSK | 20 Nov 22 13:03 MSK |
| stop         |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 20 Nov 22 18:26 MSK |                     |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 23 Nov 22 15:27 MSK | 23 Nov 22 15:27 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 23 Nov 22 15:29 MSK | 23 Nov 22 15:29 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 23 Nov 22 15:30 MSK | 23 Nov 22 15:30 MSK |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 24 Nov 22 17:04 MSK | 24 Nov 22 17:04 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 24 Nov 22 17:07 MSK | 24 Nov 22 17:07 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 24 Nov 22 21:27 MSK | 24 Nov 22 21:28 MSK |
| tunnel       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 24 Nov 22 21:31 MSK |                     |
| stop         |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 25 Nov 22 23:57 MSK |                     |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 26 Nov 22 00:02 MSK |                     |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 28 Nov 22 13:07 MSK | 28 Nov 22 13:08 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 13:12 MSK | 01 Dec 22 13:12 MSK |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 13:13 MSK | 01 Dec 22 13:13 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 13:16 MSK | 01 Dec 22 13:16 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 13:42 MSK | 01 Dec 22 13:42 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 13:45 MSK | 01 Dec 22 13:45 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 14:04 MSK | 01 Dec 22 14:04 MSK |
| tunnel       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 14:16 MSK |                     |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 14:44 MSK | 01 Dec 22 14:44 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 14:56 MSK | 01 Dec 22 14:56 MSK |
| tunnel       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 14:57 MSK |                     |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 15:49 MSK | 01 Dec 22 15:49 MSK |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 15:51 MSK | 01 Dec 22 15:51 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 16:02 MSK | 01 Dec 22 16:02 MSK |
| addons       | enable ingress                                                             | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 21:53 MSK | 01 Dec 22 21:53 MSK |
| addons       | enable ingress-dns                                                         | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 21:54 MSK | 01 Dec 22 21:54 MSK |
| tunnel       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 22:27 MSK | 01 Dec 22 22:40 MSK |
| tunnel       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 01 Dec 22 22:40 MSK |                     |
| update-check |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 14:56 MSK | 02 Dec 22 14:56 MSK |
| start        |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:11 MSK | 02 Dec 22 15:11 MSK |
| start        | --nodes 2 --cni calico                                                     | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:23 MSK |                     |
|              | --kubernetes-version=v1.24.3                                               |          |                       |         |                     |                     |
| delete       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:37 MSK | 02 Dec 22 15:37 MSK |
| start        | --kubernetes-version=v1.24.3                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:38 MSK | 02 Dec 22 15:40 MSK |
| start        | --nodes 2 --cni calico                                                     | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:46 MSK | 02 Dec 22 15:46 MSK |
|              | --kubernetes-version=v1.24.3                                               |          |                       |         |                     |                     |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:47 MSK | 02 Dec 22 15:47 MSK |
| kubectl      | -- get po -A                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:51 MSK | 02 Dec 22 15:51 MSK |
| start        | --network-plugin=cni                                                       | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:57 MSK | 02 Dec 22 15:58 MSK |
|              | --cni=calico                                                               |          |                       |         |                     |                     |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 15:59 MSK | 02 Dec 22 15:59 MSK |
| start        | --nodes 2 ‚Äìp minikube                                                      | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 16:11 MSK | 02 Dec 22 16:11 MSK |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 16:12 MSK | 02 Dec 22 16:12 MSK |
| start        | --nodes 2 ‚Äìp minikube-m02                                                  | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 16:14 MSK | 02 Dec 22 16:15 MSK |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 16:15 MSK | 02 Dec 22 16:15 MSK |
| start        | --nodes 2 --cni calico                                                     | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 17:11 MSK | 02 Dec 22 17:12 MSK |
|              | --kubernetes-version=v1.24.3                                               |          |                       |         |                     |                     |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 17:28 MSK | 02 Dec 22 17:28 MSK |
| start        | -p new --network-plugin=cni                                                | new      | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 17:43 MSK | 02 Dec 22 17:44 MSK |
|              | --cni=calico ‚Äìmodes 2                                                      |          |                       |         |                     |                     |
|              | ‚Äìkubernetes-version=v1 .24.3                                               |          |                       |         |                     |                     |
| kubectl      | -- get nodes                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 17:46 MSK | 02 Dec 22 17:46 MSK |
| kubectl      | -- get po -A                                                               | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 02 Dec 22 17:53 MSK | 02 Dec 22 17:53 MSK |
| start        | --network-plugin=cni                                                       | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 03 Dec 22 12:22 MSK |                     |
|              | --cni=calico --nodes 2                                                     |          |                       |         |                     |                     |
|              | --kubernetes-version=v1.24.0                                               |          |                       |         |                     |                     |
| delete       |                                                                            | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 03 Dec 22 12:23 MSK | 03 Dec 22 12:25 MSK |
| start        | --network-plugin=cni                                                       | minikube | DESKTOP-S3MFKMQ\admin | v1.27.0 | 03 Dec 22 12:25 MSK |                     |
|              | --cni=calico --nodes 2                                                     |          |                       |         |                     |                     |
|              | --kubernetes-version=v1.24.0                                               |          |                       |         |                     |                     |
|--------------|----------------------------------------------------------------------------|----------|-----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/12/03 12:25:48
Running on machine: DESKTOP-S3MFKMQ
Binary: Built with gc go1.19.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1203 12:25:48.022569   24148 out.go:296] Setting OutFile to fd 92 ...
I1203 12:25:48.039826   24148 out.go:348] isatty.IsTerminal(92) = true
I1203 12:25:48.039826   24148 out.go:309] Setting ErrFile to fd 96...
I1203 12:25:48.039826   24148 out.go:348] isatty.IsTerminal(96) = true
I1203 12:25:48.054023   24148 out.go:303] Setting JSON to false
I1203 12:25:48.061411   24148 start.go:115] hostinfo: {"hostname":"DESKTOP-S3MFKMQ","uptime":1592526,"bootTime":1668467022,"procs":283,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045 Build 19045","kernelVersion":"10.0.19045 Build 19045","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"175f9b5c-de38-4ea9-81d9-e03c66e167be"}
W1203 12:25:48.061411   24148 start.go:123] gopshost.Virtualization returned error: not implemented yet
I1203 12:25:48.074158   24148 out.go:177] üòÑ  minikube v1.27.0 on Microsoft Windows 10 Pro 10.0.19045 Build 19045
I1203 12:25:48.074679   24148 notify.go:214] Checking for updates...
I1203 12:25:48.075717   24148 config.go:180] Loaded profile config "new": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1203 12:25:48.076253   24148 driver.go:365] Setting default libvirt URI to qemu:///system
I1203 12:25:48.227326   24148 docker.go:137] docker version: linux-20.10.20
I1203 12:25:48.230519   24148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 12:25:48.703424   24148 info.go:265] docker info: {ID:PUJH:PALP:MOL3:QKZC:FVUF:RBUA:VPPU:4ULW:3CO2:H2SK:NGAP:6Y4R Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:true NGoroutines:50 SystemTime:2022-12-03 09:25:48.320847592 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:11588521984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1203 12:25:48.704992   24148 out.go:177] ‚ú®  Using the docker driver based on user configuration
I1203 12:25:48.706058   24148 start.go:284] selected driver: docker
I1203 12:25:48.706058   24148 start.go:808] validating driver "docker" against <nil>
I1203 12:25:48.706058   24148 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1203 12:25:48.711276   24148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 12:25:49.112447   24148 info.go:265] docker info: {ID:PUJH:PALP:MOL3:QKZC:FVUF:RBUA:VPPU:4ULW:3CO2:H2SK:NGAP:6Y4R Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:true NGoroutines:50 SystemTime:2022-12-03 09:25:48.793513655 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:11588521984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1203 12:25:49.112447   24148 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
W1203 12:25:49.112447   24148 out.go:239] ‚ùó  With --network-plugin=cni, you will need to provide your own CNI. See --cni flag as a user-friendly alternative
I1203 12:25:49.194383   24148 start_flags.go:377] Using suggested 2200MB memory alloc based on sys=14188MB, container=11051MB
I1203 12:25:49.194383   24148 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I1203 12:25:49.195484   24148 out.go:177] üìå  Using Docker Desktop driver with root privileges
I1203 12:25:49.210040   24148 cni.go:95] Creating CNI manager for "calico"
I1203 12:25:49.210040   24148 start_flags.go:305] Found "Calico" CNI - setting NetworkPlugin=cni
I1203 12:25:49.210040   24148 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1203 12:25:49.211612   24148 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1203 12:25:49.212649   24148 cache.go:120] Beginning downloading kic base image for docker with docker
I1203 12:25:49.213173   24148 out.go:177] üöú  Pulling base image ...
I1203 12:25:49.216812   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:25:49.216826   24148 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1203 12:25:49.340200   24148 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1203 12:25:49.340200   24148 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1203 12:25:49.353970   24148 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.24.0/preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4
I1203 12:25:49.353970   24148 cache.go:57] Caching tarball of preloaded images
I1203 12:25:49.353970   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:25:49.355385   24148 out.go:177] üíæ  Downloading Kubernetes v1.24.0 preload ...
I1203 12:25:49.356459   24148 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4 ...
I1203 12:25:49.879398   24148 download.go:101] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.24.0/preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4?checksum=md5:9e4c3d65e188e4489cff634ff40b94cf -> C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4
I1203 12:26:55.889783   24148 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4 ...
I1203 12:26:55.890783   24148 preload.go:256] verifying checksum of C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4 ...
I1203 12:26:56.551334   24148 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.0 on docker
I1203 12:26:56.551334   24148 profile.go:148] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I1203 12:26:56.551334   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\config.json: {Name:mk0438f7842862f34f2971e6d7deebd1156bea97 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:26:56.552336   24148 cache.go:208] Successfully downloaded all kic artifacts
I1203 12:26:56.552336   24148 start.go:364] acquiring machines lock for minikube: {Name:mk643f7e9da83601d587a510a4be0c611c47f2bc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1203 12:26:56.552336   24148 start.go:368] acquired machines lock for "minikube" in 0s
I1203 12:26:56.552336   24148 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1203 12:26:56.552336   24148 start.go:125] createHost starting for "" (driver="docker")
I1203 12:26:56.571140   24148 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1203 12:26:56.572202   24148 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1203 12:26:56.572202   24148 client.go:168] LocalClient.Create starting
I1203 12:26:56.572777   24148 main.go:134] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\ca.pem
I1203 12:26:56.578570   24148 main.go:134] libmachine: Decoding PEM data...
I1203 12:26:56.579096   24148 main.go:134] libmachine: Parsing certificate...
I1203 12:26:56.579622   24148 main.go:134] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\cert.pem
I1203 12:26:56.584911   24148 main.go:134] libmachine: Decoding PEM data...
I1203 12:26:56.584911   24148 main.go:134] libmachine: Parsing certificate...
I1203 12:26:56.588601   24148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1203 12:26:56.711592   24148 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1203 12:26:56.713686   24148 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1203 12:26:56.714246   24148 cli_runner.go:164] Run: docker network inspect minikube
W1203 12:26:56.833016   24148 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1203 12:26:56.833016   24148 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1203 12:26:56.833016   24148 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1203 12:26:56.835097   24148 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1203 12:26:57.034640   24148 network.go:290] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc000614670] misses:0}
I1203 12:26:57.034640   24148 network.go:236] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1203 12:26:57.034640   24148 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1203 12:26:57.038385   24148 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1203 12:26:58.187922   24148 cli_runner.go:217] Completed: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube: (1.1495373s)
I1203 12:26:58.187922   24148 network_create.go:99] docker network minikube 192.168.49.0/24 created
I1203 12:26:58.187922   24148 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I1203 12:26:58.193173   24148 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1203 12:26:58.312546   24148 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1203 12:26:58.433179   24148 oci.go:103] Successfully created a docker volume minikube
I1203 12:26:58.436176   24148 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1203 12:26:59.836181   24148 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib: (1.4000054s)
I1203 12:26:59.836181   24148 oci.go:107] Successfully prepared a docker volume minikube
I1203 12:26:59.836181   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:26:59.836181   24148 kic.go:179] Starting extracting preloaded images to volume ...
I1203 12:26:59.839990   24148 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
I1203 12:29:10.595461   24148 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (2m10.7554713s)
I1203 12:29:10.595461   24148 kic.go:188] duration metric: took 130.759280 seconds to extract preloaded images to volume
I1203 12:29:10.601461   24148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 12:31:10.996946   24148 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2m0.3954846s)
I1203 12:31:10.996946   24148 info.go:265] docker info: {ID:PUJH:PALP:MOL3:QKZC:FVUF:RBUA:VPPU:4ULW:3CO2:H2SK:NGAP:6Y4R Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:47 OomKillDisable:true NGoroutines:50 SystemTime:2022-12-03 09:31:10.672360052 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:11588521984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1203 12:31:10.999552   24148 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1203 12:31:11.455690   24148 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1203 12:31:14.215138   24148 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c: (2.7594472s)
I1203 12:31:14.219138   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1203 12:31:14.416312   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:31:14.583713   24148 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1203 12:31:14.892452   24148 oci.go:144] the created container "minikube" has a running status.
I1203 12:31:14.892452   24148 kic.go:210] Creating ssh key for kic: C:\Users\admin\.minikube\machines\minikube\id_rsa...
I1203 12:31:14.979661   24148 kic_runner.go:191] docker (temp): C:\Users\admin\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1203 12:31:15.412908   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:31:15.603169   24148 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1203 12:31:15.603169   24148 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1203 12:31:15.931249   24148 kic.go:250] ensuring only current user has permissions to key file located at : C:\Users\admin\.minikube\machines\minikube\id_rsa...
I1203 12:31:16.770497   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:31:16.931869   24148 machine.go:88] provisioning docker machine ...
I1203 12:31:16.931869   24148 ubuntu.go:169] provisioning hostname "minikube"
I1203 12:31:16.933870   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:17.073317   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:31:17.080318   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64586 <nil> <nil>}
I1203 12:31:17.080318   24148 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1203 12:31:17.285015   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1203 12:31:17.291016   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:17.445402   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:31:17.445402   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64586 <nil> <nil>}
I1203 12:31:17.445402   24148 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1203 12:31:17.661464   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1203 12:31:17.661464   24148 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I1203 12:31:17.661464   24148 ubuntu.go:177] setting up certificates
I1203 12:31:17.661464   24148 provision.go:83] configureAuth start
I1203 12:31:17.663463   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 12:31:17.814918   24148 provision.go:138] copyHostCerts
I1203 12:31:17.815915   24148 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I1203 12:31:17.815915   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\ca.pem
I1203 12:31:17.815915   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I1203 12:31:17.816913   24148 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I1203 12:31:17.816913   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\cert.pem
I1203 12:31:17.816913   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I1203 12:31:17.821911   24148 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I1203 12:31:17.821911   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\key.pem
I1203 12:31:17.821911   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1679 bytes)
I1203 12:31:17.822914   24148 provision.go:112] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1203 12:31:17.873912   24148 provision.go:172] copyRemoteCerts
I1203 12:31:17.884914   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1203 12:31:17.888914   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:18.047104   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:31:18.184652   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1203 12:31:18.244693   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I1203 12:31:18.294426   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1203 12:31:18.363004   24148 provision.go:86] duration metric: configureAuth took 700.5402ms
I1203 12:31:18.363004   24148 ubuntu.go:193] setting minikube options for container-runtime
I1203 12:31:18.363004   24148 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.0
I1203 12:31:18.365004   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:18.521600   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:31:18.522599   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64586 <nil> <nil>}
I1203 12:31:18.522599   24148 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1203 12:31:18.712221   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1203 12:31:18.712221   24148 ubuntu.go:71] root file system type: overlay
I1203 12:31:18.712221   24148 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1203 12:31:18.714863   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:18.885550   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:31:18.885550   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64586 <nil> <nil>}
I1203 12:31:18.886551   24148 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1203 12:31:19.092073   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1203 12:31:19.095072   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:19.254286   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:31:19.254286   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64586 <nil> <nil>}
I1203 12:31:19.254286   24148 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1203 12:31:23.205449   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-12-03 09:31:19.083853824 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1203 12:31:23.205449   24148 machine.go:91] provisioned docker machine in 6.27358s
I1203 12:31:23.205449   24148 client.go:171] LocalClient.Create took 4m26.6332472s
I1203 12:31:23.205449   24148 start.go:167] duration metric: libmachine.API.Create for "minikube" took 4m26.6332472s
I1203 12:31:23.205449   24148 start.go:300] post-start starting for "minikube" (driver="docker")
I1203 12:31:23.205449   24148 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1203 12:31:23.214446   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1203 12:31:23.218448   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:23.386236   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:31:23.530242   24148 ssh_runner.go:195] Run: cat /etc/os-release
I1203 12:31:23.534420   24148 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1203 12:31:23.534420   24148 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1203 12:31:23.534420   24148 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1203 12:31:23.534420   24148 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1203 12:31:23.534420   24148 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I1203 12:31:23.534948   24148 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I1203 12:31:23.535471   24148 start.go:303] post-start completed in 330.0216ms
I1203 12:31:23.541070   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 12:31:23.676034   24148 profile.go:148] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I1203 12:31:23.683032   24148 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1203 12:31:23.686036   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:23.814679   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:31:23.946534   24148 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1203 12:31:23.954726   24148 start.go:128] duration metric: createHost completed in 4m27.4023906s
I1203 12:31:23.954726   24148 start.go:83] releasing machines lock for "minikube", held for 4m27.4023906s
I1203 12:31:23.957403   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1203 12:31:24.106400   24148 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1203 12:31:24.110399   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:24.112399   24148 ssh_runner.go:195] Run: systemctl --version
I1203 12:31:24.114403   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:31:24.356154   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:31:24.371588   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:31:24.484114   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1203 12:31:24.774308   24148 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (233 bytes)
I1203 12:31:24.813305   24148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 12:31:24.963706   24148 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1203 12:31:25.174206   24148 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1203 12:31:25.195387   24148 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1203 12:31:25.211387   24148 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1203 12:31:25.233387   24148 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1203 12:31:25.282387   24148 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1203 12:31:25.445390   24148 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1203 12:31:25.634348   24148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 12:31:25.792642   24148 ssh_runner.go:195] Run: sudo systemctl restart docker
I1203 12:31:29.333386   24148 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.5407438s)
I1203 12:31:29.343378   24148 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1203 12:31:29.525171   24148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1203 12:31:29.682639   24148 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1203 12:31:29.711668   24148 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1203 12:31:29.721670   24148 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1203 12:31:29.731668   24148 start.go:471] Will wait 60s for crictl version
I1203 12:31:29.742669   24148 ssh_runner.go:195] Run: sudo crictl version
I1203 12:31:29.823931   24148 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1203 12:31:29.828653   24148 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1203 12:31:29.923391   24148 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1203 12:31:30.013011   24148 out.go:204] üê≥  Preparing Kubernetes v1.24.0 on Docker 20.10.17 ...
I1203 12:31:30.016013   24148 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1203 12:31:30.374486   24148 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1203 12:31:30.383485   24148 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1203 12:31:30.392485   24148 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1203 12:31:30.425997   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1203 12:31:30.574605   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:31:30.580124   24148 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1203 12:31:30.651620   24148 docker.go:611] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.0
k8s.gcr.io/kube-proxy:v1.24.0
k8s.gcr.io/kube-controller-manager:v1.24.0
k8s.gcr.io/kube-scheduler:v1.24.0
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1203 12:31:30.651620   24148 docker.go:542] Images already preloaded, skipping extraction
I1203 12:31:30.654311   24148 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1203 12:31:30.742564   24148 docker.go:611] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.0
k8s.gcr.io/kube-proxy:v1.24.0
k8s.gcr.io/kube-controller-manager:v1.24.0
k8s.gcr.io/kube-scheduler:v1.24.0
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1203 12:31:30.742564   24148 cache_images.go:84] Images are preloaded, skipping loading
I1203 12:31:30.744700   24148 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1203 12:31:30.933686   24148 cni.go:95] Creating CNI manager for "calico"
I1203 12:31:30.933686   24148 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1203 12:31:30.933686   24148 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.24.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1203 12:31:30.933686   24148 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1203 12:31:30.933686   24148 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:}
I1203 12:31:30.944686   24148 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.0
I1203 12:31:30.982235   24148 binaries.go:44] Found k8s binaries, skipping transfer
I1203 12:31:30.992920   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1203 12:31:31.020160   24148 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1203 12:31:31.051905   24148 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1203 12:31:31.073422   24148 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2031 bytes)
I1203 12:31:31.111790   24148 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1203 12:31:31.115008   24148 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1203 12:31:31.135813   24148 certs.go:54] Setting up C:\Users\admin\.minikube\profiles\minikube for IP: 192.168.49.2
I1203 12:31:31.153810   24148 certs.go:182] skipping minikubeCA CA generation: C:\Users\admin\.minikube\ca.key
I1203 12:31:31.164532   24148 certs.go:182] skipping proxyClientCA CA generation: C:\Users\admin\.minikube\proxy-client-ca.key
I1203 12:31:31.164532   24148 certs.go:302] generating minikube-user signed cert: C:\Users\admin\.minikube\profiles\minikube\client.key
I1203 12:31:31.164532   24148 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\client.crt with IP's: []
I1203 12:31:31.425541   24148 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\client.crt ...
I1203 12:31:31.425541   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\client.crt: {Name:mk1c354620d23d1db4d5f75cf7680da6ff84fa10 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.426542   24148 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\client.key ...
I1203 12:31:31.426542   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\client.key: {Name:mk6fd8ed109c6eb3817685a7b3c3510425de957a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.427547   24148 certs.go:302] generating minikube signed cert: C:\Users\admin\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1203 12:31:31.427547   24148 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1203 12:31:31.474625   24148 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 ...
I1203 12:31:31.474625   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2: {Name:mke6496cf9d566af47bd498230d321728df5cc16 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.474625   24148 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 ...
I1203 12:31:31.474625   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\apiserver.key.dd3b5fb2: {Name:mk60959c1d6e487cf0cda0f2ce93a2be150bacd6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.475628   24148 certs.go:320] copying C:\Users\admin\.minikube\profiles\minikube\apiserver.crt.dd3b5fb2 -> C:\Users\admin\.minikube\profiles\minikube\apiserver.crt
I1203 12:31:31.478625   24148 certs.go:324] copying C:\Users\admin\.minikube\profiles\minikube\apiserver.key.dd3b5fb2 -> C:\Users\admin\.minikube\profiles\minikube\apiserver.key
I1203 12:31:31.479626   24148 certs.go:302] generating aggregator signed cert: C:\Users\admin\.minikube\profiles\minikube\proxy-client.key
I1203 12:31:31.479626   24148 crypto.go:68] Generating cert C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1203 12:31:31.521218   24148 crypto.go:156] Writing cert to C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt ...
I1203 12:31:31.521218   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt: {Name:mk95af3ec4c02cfa13d7d53e2939b9964b736a77 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.521218   24148 crypto.go:164] Writing key to C:\Users\admin\.minikube\profiles\minikube\proxy-client.key ...
I1203 12:31:31.521218   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.minikube\profiles\minikube\proxy-client.key: {Name:mkda28e207fc620d768ca92b69e7df85c52a9e02 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:31:31.526828   24148 certs.go:388] found cert: C:\Users\admin\.minikube\certs\C:\Users\admin\.minikube\certs\ca-key.pem (1679 bytes)
I1203 12:31:31.526828   24148 certs.go:388] found cert: C:\Users\admin\.minikube\certs\C:\Users\admin\.minikube\certs\ca.pem (1074 bytes)
I1203 12:31:31.526828   24148 certs.go:388] found cert: C:\Users\admin\.minikube\certs\C:\Users\admin\.minikube\certs\cert.pem (1119 bytes)
I1203 12:31:31.526828   24148 certs.go:388] found cert: C:\Users\admin\.minikube\certs\C:\Users\admin\.minikube\certs\key.pem (1679 bytes)
I1203 12:31:31.528016   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1203 12:31:31.580823   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1203 12:31:31.631691   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1203 12:31:31.682337   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1203 12:31:31.724917   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1203 12:31:31.782059   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1203 12:31:31.833285   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1203 12:31:31.881699   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1203 12:31:31.913734   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1203 12:31:31.952527   24148 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1203 12:31:32.000116   24148 ssh_runner.go:195] Run: openssl version
I1203 12:31:32.020116   24148 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1203 12:31:32.041116   24148 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1203 12:31:32.045116   24148 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Sep 30 15:43 /usr/share/ca-certificates/minikubeCA.pem
I1203 12:31:32.051117   24148 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1203 12:31:32.074115   24148 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1203 12:31:32.102116   24148 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1203 12:31:32.104116   24148 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1203 12:31:32.183115   24148 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1203 12:31:32.223530   24148 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1203 12:31:32.242662   24148 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I1203 12:31:32.251692   24148 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1203 12:31:32.275207   24148 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1203 12:31:32.275207   24148 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1203 12:31:32.353529   24148 kubeadm.go:317] W1203 09:31:32.352771    1210 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
I1203 12:31:32.453079   24148 kubeadm.go:317] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I1203 12:31:32.631758   24148 kubeadm.go:317] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1203 12:32:08.215418   24148 kubeadm.go:317] [init] Using Kubernetes version: v1.24.0
I1203 12:32:08.215418   24148 kubeadm.go:317] [preflight] Running pre-flight checks
I1203 12:32:08.215418   24148 kubeadm.go:317] [preflight] Pulling images required for setting up a Kubernetes cluster
I1203 12:32:08.215418   24148 kubeadm.go:317] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1203 12:32:08.215418   24148 kubeadm.go:317] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I1203 12:32:08.215418   24148 kubeadm.go:317] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1203 12:32:08.218413   24148 out.go:204]     ‚ñ™ Generating certificates and keys ...
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Using existing ca certificate authority
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Using existing apiserver certificate and key on disk
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Generating "apiserver-kubelet-client" certificate and key
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Generating "front-proxy-ca" certificate and key
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Generating "front-proxy-client" certificate and key
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Generating "etcd/ca" certificate and key
I1203 12:32:08.219413   24148 kubeadm.go:317] [certs] Generating "etcd/server" certificate and key
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] Generating "etcd/peer" certificate and key
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] Generating "etcd/healthcheck-client" certificate and key
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] Generating "apiserver-etcd-client" certificate and key
I1203 12:32:08.220413   24148 kubeadm.go:317] [certs] Generating "sa" key and public key
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubeconfig] Writing "admin.conf" kubeconfig file
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1203 12:32:08.220413   24148 kubeadm.go:317] [kubelet-start] Starting the kubelet
I1203 12:32:08.220413   24148 kubeadm.go:317] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1203 12:32:08.221415   24148 out.go:204]     ‚ñ™ Booting up control plane ...
I1203 12:32:08.221415   24148 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1203 12:32:08.222413   24148 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1203 12:32:08.222413   24148 kubeadm.go:317] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1203 12:32:08.222413   24148 kubeadm.go:317] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1203 12:32:08.222413   24148 kubeadm.go:317] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I1203 12:32:08.222413   24148 kubeadm.go:317] [apiclient] All control plane components are healthy after 28.518425 seconds
I1203 12:32:08.222413   24148 kubeadm.go:317] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1203 12:32:08.222413   24148 kubeadm.go:317] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1203 12:32:08.222413   24148 kubeadm.go:317] [upload-certs] Skipping phase. Please see --upload-certs
I1203 12:32:08.223412   24148 kubeadm.go:317] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1203 12:32:08.223412   24148 kubeadm.go:317] [bootstrap-token] Using token: dcm3vt.bz4npx9tz2uy869q
I1203 12:32:08.224416   24148 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I1203 12:32:08.224416   24148 kubeadm.go:317] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1203 12:32:08.224416   24148 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1203 12:32:08.224416   24148 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1203 12:32:08.225416   24148 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1203 12:32:08.225416   24148 kubeadm.go:317] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1203 12:32:08.225416   24148 kubeadm.go:317] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1203 12:32:08.225416   24148 kubeadm.go:317] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1203 12:32:08.225416   24148 kubeadm.go:317] [addons] Applied essential addon: CoreDNS
I1203 12:32:08.225416   24148 kubeadm.go:317] [addons] Applied essential addon: kube-proxy
I1203 12:32:08.225416   24148 kubeadm.go:317] 
I1203 12:32:08.225416   24148 kubeadm.go:317] Your Kubernetes control-plane has initialized successfully!
I1203 12:32:08.225416   24148 kubeadm.go:317] 
I1203 12:32:08.225416   24148 kubeadm.go:317] To start using your cluster, you need to run the following as a regular user:
I1203 12:32:08.225416   24148 kubeadm.go:317] 
I1203 12:32:08.225416   24148 kubeadm.go:317]   mkdir -p $HOME/.kube
I1203 12:32:08.225416   24148 kubeadm.go:317]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1203 12:32:08.226415   24148 kubeadm.go:317]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317] Alternatively, if you are the root user, you can run:
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317] You should now deploy a pod network to the cluster.
I1203 12:32:08.226415   24148 kubeadm.go:317] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1203 12:32:08.226415   24148 kubeadm.go:317]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317] You can now join any number of control-plane nodes by copying certificate authorities
I1203 12:32:08.226415   24148 kubeadm.go:317] and service account keys on each node and then running the following as root:
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317]   kubeadm join control-plane.minikube.internal:8443 --token dcm3vt.bz4npx9tz2uy869q \
I1203 12:32:08.226415   24148 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:9414c3c02567dbb7e280ccbf8c862e3d2df0f1e1763440b1cd6dead4f301c104 \
I1203 12:32:08.226415   24148 kubeadm.go:317] 	--control-plane 
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.226415   24148 kubeadm.go:317] Then you can join any number of worker nodes by running the following on each as root:
I1203 12:32:08.226415   24148 kubeadm.go:317] 
I1203 12:32:08.227414   24148 kubeadm.go:317] kubeadm join control-plane.minikube.internal:8443 --token dcm3vt.bz4npx9tz2uy869q \
I1203 12:32:08.227414   24148 kubeadm.go:317] 	--discovery-token-ca-cert-hash sha256:9414c3c02567dbb7e280ccbf8c862e3d2df0f1e1763440b1cd6dead4f301c104 
I1203 12:32:08.227414   24148 cni.go:95] Creating CNI manager for "calico"
I1203 12:32:08.228414   24148 out.go:177] üîó  Configuring Calico (Container Networking Interface) ...
I1203 12:32:08.230416   24148 cni.go:189] applying CNI manifest using /var/lib/minikube/binaries/v1.24.0/kubectl ...
I1203 12:32:08.230416   24148 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (202050 bytes)
I1203 12:32:08.732547   24148 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I1203 12:32:16.432664   24148 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.24.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (7.7001164s)
I1203 12:32:16.432664   24148 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1203 12:32:16.444014   24148 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1203 12:32:16.445592   24148 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.0/kubectl label nodes minikube.k8s.io/version=v1.27.0 minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_12_03T12_32_16_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1203 12:32:16.454192   24148 ops.go:34] apiserver oom_adj: -16
I1203 12:32:16.825085   24148 kubeadm.go:1067] duration metric: took 392.4216ms to wait for elevateKubeSystemPrivileges.
I1203 12:32:17.215804   24148 kubeadm.go:398] StartCluster complete in 45.1136879s
I1203 12:32:17.215804   24148 settings.go:142] acquiring lock: {Name:mk5c419c3fabff3b09a504acb297c33a24a93937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:32:17.216121   24148 settings.go:150] Updating kubeconfig:  C:\Users\admin\.kube\config
I1203 12:32:17.218153   24148 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1203 12:32:18.043588   24148 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1203 12:32:18.043588   24148 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1203 12:32:18.043588   24148 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1203 12:32:18.044589   24148 out.go:177] üîé  Verifying Kubernetes components...
I1203 12:32:18.043588   24148 addons.go:412] enableAddons start: toEnable=map[], additional=[]
I1203 12:32:18.044589   24148 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.0
I1203 12:32:18.044589   24148 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1203 12:32:18.044589   24148 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1203 12:32:18.044589   24148 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W1203 12:32:18.044589   24148 addons.go:162] addon storage-provisioner should already be in state true
I1203 12:32:18.044589   24148 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1203 12:32:18.044589   24148 host.go:66] Checking if "minikube" exists ...
I1203 12:32:18.051589   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:32:18.052593   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:32:18.055591   24148 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1203 12:32:18.242526   24148 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1203 12:32:18.243527   24148 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1203 12:32:18.243527   24148 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1203 12:32:18.247528   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:32:18.323031   24148 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1203 12:32:18.323031   24148 addons.go:162] addon default-storageclass should already be in state true
I1203 12:32:18.323577   24148 host.go:66] Checking if "minikube" exists ...
I1203 12:32:18.333168   24148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1203 12:32:18.426016   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:32:18.519016   24148 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1203 12:32:18.519016   24148 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1203 12:32:18.523019   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1203 12:32:18.685244   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64586 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I1203 12:32:18.741833   24148 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.24.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1203 12:32:18.744833   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1203 12:32:18.900849   24148 api_server.go:51] waiting for apiserver process to appear ...
I1203 12:32:18.911851   24148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1203 12:32:19.142064   24148 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1203 12:32:19.633623   24148 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1203 12:32:34.015221   24148 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.24.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (15.2733879s)
I1203 12:32:34.015221   24148 start.go:810] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS
I1203 12:32:34.015221   24148 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (15.1033697s)
I1203 12:32:34.015221   24148 api_server.go:71] duration metric: took 15.9716328s to wait for apiserver process to appear ...
I1203 12:32:34.015221   24148 api_server.go:87] waiting for apiserver healthz status ...
I1203 12:32:34.015221   24148 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:64590/healthz ...
I1203 12:32:34.212834   24148 api_server.go:266] https://127.0.0.1:64590/healthz returned 200:
ok
I1203 12:32:34.215833   24148 api_server.go:140] control plane version: v1.24.0
I1203 12:32:34.215833   24148 api_server.go:130] duration metric: took 200.6122ms to wait for apiserver health ...
I1203 12:32:34.215833   24148 system_pods.go:43] waiting for kube-system pods to appear ...
I1203 12:32:34.430310   24148 system_pods.go:59] 8 kube-system pods found
I1203 12:32:34.430310   24148 system_pods.go:61] "calico-kube-controllers-c44b4545-879dj" [3464ab0c-c505-4ad1-8cf3-1f488a572990] Pending / Ready:ContainersNotReady (containers with unready status: [calico-kube-controllers]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-kube-controllers])
I1203 12:32:34.430310   24148 system_pods.go:61] "calico-node-jjx6b" [a31f9b11-18c6-4a9c-8a70-4642caecb8cb] Pending: Initialized:ContainersNotInitialized (containers with incomplete status: [upgrade-ipam install-cni flexvol-driver]) / Ready:ContainersNotReady (containers with unready status: [calico-node]) / ContainersReady:ContainersNotReady (containers with unready status: [calico-node])
I1203 12:32:34.430310   24148 system_pods.go:61] "coredns-6d4b75cb6d-sgpn2" [81687801-71ea-469a-9e13-deb56085c583] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1203 12:32:34.430310   24148 system_pods.go:61] "etcd-minikube" [2e7c68a6-2d61-4e4d-beb5-492fdd633616] Running
I1203 12:32:34.430310   24148 system_pods.go:61] "kube-apiserver-minikube" [36e696c0-35d3-4469-808b-96a07fc54e9e] Running
I1203 12:32:34.430310   24148 system_pods.go:61] "kube-controller-manager-minikube" [7fa78bf3-442d-4825-9772-c73c84dc3210] Running
I1203 12:32:34.430310   24148 system_pods.go:61] "kube-proxy-sg5jc" [49325fbb-7b1a-4aa3-9ac8-37785545be07] Running
I1203 12:32:34.430310   24148 system_pods.go:61] "kube-scheduler-minikube" [3335a829-600d-4fab-9016-fcc002c8e854] Running
I1203 12:32:34.430310   24148 system_pods.go:74] duration metric: took 214.4774ms to wait for pod list to return data ...
I1203 12:32:34.430310   24148 kubeadm.go:573] duration metric: took 16.3867224s to wait for : map[apiserver:true system_pods:true] ...
I1203 12:32:34.430310   24148 node_conditions.go:102] verifying NodePressure condition ...
I1203 12:32:34.524616   24148 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I1203 12:32:34.524616   24148 node_conditions.go:123] node cpu capacity is 12
I1203 12:32:34.524616   24148 node_conditions.go:105] duration metric: took 94.3061ms to run NodePressure ...
I1203 12:32:34.524616   24148 start.go:216] waiting for startup goroutines ...
I1203 12:32:34.923191   24148 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (15.781127s)
I1203 12:32:34.923191   24148 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (15.2895679s)
I1203 12:32:34.924194   24148 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1203 12:32:34.925194   24148 addons.go:414] enableAddons completed in 16.8816066s
I1203 12:32:34.928194   24148 out.go:177] 
I1203 12:32:34.935192   24148 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.0
I1203 12:32:34.936192   24148 config.go:180] Loaded profile config "new": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.0
I1203 12:32:34.936192   24148 profile.go:148] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I1203 12:32:34.939191   24148 out.go:177] üëç  Starting worker node minikube-m02 in cluster minikube
I1203 12:32:34.940191   24148 cache.go:120] Beginning downloading kic base image for docker with docker
I1203 12:32:34.940191   24148 out.go:177] üöú  Pulling base image ...
I1203 12:32:34.941191   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:32:34.941191   24148 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon
I1203 12:32:34.941191   24148 cache.go:57] Caching tarball of preloaded images
I1203 12:32:34.942192   24148 preload.go:174] Found C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1203 12:32:34.942192   24148 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.0 on docker
I1203 12:32:34.942192   24148 profile.go:148] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I1203 12:32:35.130563   24148 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c in local docker daemon, skipping pull
I1203 12:32:35.130563   24148 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c exists in daemon, skipping load
I1203 12:32:35.130563   24148 cache.go:208] Successfully downloaded all kic artifacts
I1203 12:32:35.130563   24148 start.go:364] acquiring machines lock for minikube-m02: {Name:mka7a25609c9581c5bae67c552ab2caccc8d457a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1203 12:32:35.131089   24148 start.go:368] acquired machines lock for "minikube-m02" in 0s
I1203 12:32:35.131089   24148 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.0 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:calico NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:0 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name:m02 IP: Port:0 KubernetesVersion:v1.24.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I1203 12:32:35.131089   24148 start.go:125] createHost starting for "m02" (driver="docker")
I1203 12:32:35.132135   24148 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I1203 12:32:35.132666   24148 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1203 12:32:35.132666   24148 client.go:168] LocalClient.Create starting
I1203 12:32:35.132666   24148 main.go:134] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\ca.pem
I1203 12:32:35.132666   24148 main.go:134] libmachine: Decoding PEM data...
I1203 12:32:35.133187   24148 main.go:134] libmachine: Parsing certificate...
I1203 12:32:35.133187   24148 main.go:134] libmachine: Reading certificate data from C:\Users\admin\.minikube\certs\cert.pem
I1203 12:32:35.133187   24148 main.go:134] libmachine: Decoding PEM data...
I1203 12:32:35.133187   24148 main.go:134] libmachine: Parsing certificate...
I1203 12:32:35.139341   24148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1203 12:32:35.301077   24148 network_create.go:76] Found existing network {name:minikube subnet:0xc000d50000 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I1203 12:32:35.301077   24148 kic.go:106] calculated static IP "192.168.49.3" for the "minikube-m02" container
I1203 12:32:35.307075   24148 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1203 12:32:35.504833   24148 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I1203 12:32:35.786317   24148 oci.go:103] Successfully created a docker volume minikube-m02
I1203 12:32:35.791375   24148 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib
I1203 12:32:38.340976   24148 cli_runner.go:217] Completed: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -d /var/lib: (2.549601s)
I1203 12:32:38.340976   24148 oci.go:107] Successfully prepared a docker volume minikube-m02
I1203 12:32:38.340976   24148 preload.go:132] Checking if preload exists for k8s version v1.24.0 and runtime docker
I1203 12:32:38.340976   24148 kic.go:179] Starting extracting preloaded images to volume ...
I1203 12:32:38.343976   24148 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir
W1203 12:34:38.529329   24148 cli_runner.go:211] docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir returned with exit code 125
I1203 12:34:38.529329   24148 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: (2m0.1853533s)
I1203 12:34:38.529329   24148 kic.go:186] Unable to extract preloaded tarball to volume: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.24.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c -I lz4 -xf /preloaded.tar -C /extractDir: exit status 125
stdout:

stderr:
docker: Error response from daemon: i/o timeout.
See 'docker run --help'.
I1203 12:34:38.533330   24148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1203 12:34:39.101840   24148 info.go:265] docker info: {ID:PUJH:PALP:MOL3:QKZC:FVUF:RBUA:VPPU:4ULW:3CO2:H2SK:NGAP:6Y4R Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:55 SystemTime:2022-12-03 09:34:38.659195096 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:11588521984 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.20 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1203 12:34:39.103918   24148 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1203 12:34:39.654002   24148 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c
I1203 12:34:43.731400   24148 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.34@sha256:f2a1e577e43fd6769f35cdb938f6d21c3dacfd763062d119cade738fa244720c: (4.077385s)
I1203 12:34:43.736186   24148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I1203 12:34:43.976760   24148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1203 12:34:44.175525   24148 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I1203 12:34:44.575710   24148 oci.go:144] the created container "minikube-m02" has a running status.
I1203 12:34:44.575710   24148 kic.go:210] Creating ssh key for kic: C:\Users\admin\.minikube\machines\minikube-m02\id_rsa...
I1203 12:34:44.683429   24148 kic_runner.go:191] docker (temp): C:\Users\admin\.minikube\machines\minikube-m02\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1203 12:34:45.100525   24148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1203 12:34:45.325186   24148 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1203 12:34:45.325186   24148 kic_runner.go:114] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I1203 12:34:45.783141   24148 kic.go:250] ensuring only current user has permissions to key file located at : C:\Users\admin\.minikube\machines\minikube-m02\id_rsa...
I1203 12:34:46.241343   24148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I1203 12:34:46.424015   24148 machine.go:88] provisioning docker machine ...
I1203 12:34:46.424015   24148 ubuntu.go:169] provisioning hostname "minikube-m02"
I1203 12:34:46.430016   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:46.614535   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:34:46.623533   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64739 <nil> <nil>}
I1203 12:34:46.623533   24148 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I1203 12:34:46.953499   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube-m02

I1203 12:34:46.956499   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:47.169060   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:34:47.169060   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64739 <nil> <nil>}
I1203 12:34:47.169060   24148 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I1203 12:34:47.424241   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1203 12:34:47.424241   24148 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I1203 12:34:47.424241   24148 ubuntu.go:177] setting up certificates
I1203 12:34:47.424241   24148 provision.go:83] configureAuth start
I1203 12:34:47.429243   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1203 12:34:47.613324   24148 provision.go:138] copyHostCerts
I1203 12:34:47.613324   24148 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I1203 12:34:47.613324   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\ca.pem
I1203 12:34:47.613324   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I1203 12:34:47.614326   24148 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I1203 12:34:47.614326   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\cert.pem
I1203 12:34:47.614326   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I1203 12:34:47.615325   24148 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I1203 12:34:47.615325   24148 exec_runner.go:207] rm: C:\Users\admin\.minikube\key.pem
I1203 12:34:47.615325   24148 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1679 bytes)
I1203 12:34:47.616325   24148 provision.go:112] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I1203 12:34:47.845315   24148 provision.go:172] copyRemoteCerts
I1203 12:34:47.855316   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1203 12:34:47.859373   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:48.063075   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64739 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1203 12:34:48.253012   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1203 12:34:48.303179   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I1203 12:34:48.374735   24148 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1203 12:34:48.472782   24148 provision.go:86] duration metric: configureAuth took 1.0485419s
I1203 12:34:48.472782   24148 ubuntu.go:193] setting minikube options for container-runtime
I1203 12:34:48.472782   24148 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.0
I1203 12:34:48.474782   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:48.668190   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:34:48.669190   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64739 <nil> <nil>}
I1203 12:34:48.669190   24148 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1203 12:34:48.953863   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1203 12:34:48.953863   24148 ubuntu.go:71] root file system type: overlay
I1203 12:34:48.953863   24148 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1203 12:34:48.956864   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:49.143661   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:34:49.143661   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64739 <nil> <nil>}
I1203 12:34:49.143661   24148 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1203 12:34:49.446321   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1203 12:34:49.450320   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:49.635270   24148 main.go:134] libmachine: Using SSH client type: native
I1203 12:34:49.635270   24148 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x11ea860] 0x11ed7e0 <nil>  [] 0s} 127.0.0.1 64739 <nil> <nil>}
I1203 12:34:49.635270   24148 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1203 12:34:55.283862   24148 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-12-03 09:34:49.428291821 +0000
@@ -1,30 +1,33 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+Environment=NO_PROXY=192.168.49.2
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +35,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1203 12:34:55.283862   24148 machine.go:91] provisioned docker machine in 8.8598476s
I1203 12:34:55.283862   24148 client.go:171] LocalClient.Create took 2m20.1511964s
I1203 12:34:55.283862   24148 start.go:167] duration metric: libmachine.API.Create for "minikube" took 2m20.1511964s
I1203 12:34:55.283862   24148 start.go:300] post-start starting for "minikube-m02" (driver="docker")
I1203 12:34:55.283862   24148 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1203 12:34:55.296863   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1203 12:34:55.300864   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:55.512728   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64739 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1203 12:34:55.762453   24148 ssh_runner.go:195] Run: cat /etc/os-release
I1203 12:34:55.772454   24148 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1203 12:34:55.772454   24148 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1203 12:34:55.772454   24148 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1203 12:34:55.772454   24148 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1203 12:34:55.773454   24148 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I1203 12:34:55.773454   24148 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I1203 12:34:55.773454   24148 start.go:303] post-start completed in 489.5915ms
I1203 12:34:55.779454   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1203 12:34:55.976106   24148 profile.go:148] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I1203 12:34:55.983988   24148 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1203 12:34:55.986638   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:56.163417   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64739 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube-m02\id_rsa Username:docker}
I1203 12:34:56.342935   24148 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1203 12:34:56.353933   24148 start.go:128] duration metric: createHost completed in 2m21.2228448s
I1203 12:34:56.353933   24148 start.go:83] releasing machines lock for "minikube-m02", held for 2m21.2228448s
I1203 12:34:56.360937   24148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I1203 12:34:56.529452   24148 out.go:177] üåê  Found network options:
I1203 12:34:56.530452   24148 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W1203 12:34:56.532452   24148 proxy.go:119] fail to check proxy env: Error ip not in block
I1203 12:34:56.532452   24148 out.go:177]     ‚ñ™ no_proxy=192.168.49.2
W1203 12:34:56.533450   24148 proxy.go:119] fail to check proxy env: Error ip not in block
W1203 12:34:56.533450   24148 proxy.go:119] fail to check proxy env: Error ip not in block
I1203 12:34:56.534454   24148 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1203 12:34:56.539451   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:56.545465   24148 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1203 12:34:56.549464   24148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I1203 12:34:56.726193   24148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64739 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube-m02\id_rsa Username:docker}
W1203 12:36:56.719878   24148 cli_runner.go:211] docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02 returned with exit code 1
I1203 12:36:56.719878   24148 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02: (2m0.1703833s)
E1203 12:36:56.722012   24148 out.go:453] unable to execute failed to create directory: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: get port 22 for "minikube-m02": docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02: exit status 1
stdout:


stderr:
Error response from daemon: i/o timeout
: template: failed to create directory: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: get port 22 for "minikube-m02": docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02: exit status 1
stdout:


stderr:
Error response from daemon: i/o timeout
:1:231: executing "failed to create directory: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: get port 22 for \"minikube-m02\": docker container inspect -f \"'{{(index (index .NetworkSettings.Ports \"22/tcp\") 0).HostPort}}'\" minikube-m02: exit status 1\nstdout:\n\n\nstderr:\nError response from daemon: i/o timeout\n" at <index .NetworkSettings.Ports "22/tcp">: error calling index: index of untyped nil - returning raw string.
I1203 12:36:56.724646   24148 out.go:177] 
W1203 12:36:56.725685   24148 out.go:239] ‚ùå  Exiting due to RUNTIME_ENABLE: failed to create directory: NewSession: new client: new client: Error creating new ssh host from driver: Error getting ssh port for driver: get ssh host-port: get port 22 for "minikube-m02": docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02: exit status 1
stdout:


stderr:
Error response from daemon: i/o timeout

W1203 12:36:56.727243   24148 out.go:239] 
W1203 12:36:56.729866   24148 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I1203 12:36:56.732897   24148 out.go:177] 

* 
* ==> Docker <==
* -- Logs begin at Sat 2022-12-03 09:31:14 UTC, end at Sat 2022-12-03 09:42:42 UTC. --
Dec 03 09:31:25 minikube dockerd[515]: time="2022-12-03T09:31:25.823519316Z" level=info msg="Daemon shutdown complete"
Dec 03 09:31:25 minikube dockerd[515]: time="2022-12-03T09:31:25.823577099Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Dec 03 09:31:25 minikube systemd[1]: docker.service: Succeeded.
Dec 03 09:31:25 minikube systemd[1]: Stopped Docker Application Container Engine.
Dec 03 09:31:25 minikube systemd[1]: Starting Docker Application Container Engine...
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.913867722Z" level=info msg="Starting up"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.922275189Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.922310495Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.922341794Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.922350550Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.923256487Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.923280472Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.923292224Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.923297965Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.953019012Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964769686Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964796135Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964801044Z" level=warning msg="Your kernel does not support cgroup blkio throttle.read_bps_device"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964804882Z" level=warning msg="Your kernel does not support cgroup blkio throttle.write_bps_device"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964808248Z" level=warning msg="Your kernel does not support cgroup blkio throttle.read_iops_device"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964811284Z" level=warning msg="Your kernel does not support cgroup blkio throttle.write_iops_device"
Dec 03 09:31:25 minikube dockerd[810]: time="2022-12-03T09:31:25.964962587Z" level=info msg="Loading containers: start."
Dec 03 09:31:28 minikube dockerd[810]: time="2022-12-03T09:31:28.184186984Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Dec 03 09:31:29 minikube dockerd[810]: time="2022-12-03T09:31:29.241591691Z" level=info msg="Loading containers: done."
Dec 03 09:31:29 minikube dockerd[810]: time="2022-12-03T09:31:29.281930330Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Dec 03 09:31:29 minikube dockerd[810]: time="2022-12-03T09:31:29.282014547Z" level=info msg="Daemon has completed initialization"
Dec 03 09:31:29 minikube systemd[1]: Started Docker Application Container Engine.
Dec 03 09:31:29 minikube dockerd[810]: time="2022-12-03T09:31:29.342198411Z" level=info msg="API listen on [::]:2376"
Dec 03 09:31:29 minikube dockerd[810]: time="2022-12-03T09:31:29.344656034Z" level=info msg="API listen on /var/run/docker.sock"
Dec 03 09:32:33 minikube dockerd[810]: time="2022-12-03T09:32:33.812426758Z" level=info msg="ignoring event" container=b1da077828e2ff6fabb10aee76fd71377dbe49a1538d2aec2fe5852fc2f4b357 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:32:34 minikube dockerd[810]: time="2022-12-03T09:32:34.612575577Z" level=info msg="ignoring event" container=45fe1b01f30850f48c2e9593ca2c73894b7b4f3d86f21f39e9b6f74ea9405c9f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:32:45 minikube dockerd[810]: time="2022-12-03T09:32:45.222983395Z" level=info msg="ignoring event" container=15506c7331b15106e287d4c7eaeaab0c0531ac5d5ddcc2a0f82bdc509ed122e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:32:46 minikube dockerd[810]: time="2022-12-03T09:32:46.413380179Z" level=info msg="ignoring event" container=eee42a6a203b4d1791d8c1ca87e0e7d8067ad8f3b01986598f9765ec34637a65 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:32:56 minikube dockerd[810]: time="2022-12-03T09:32:56.422103683Z" level=info msg="ignoring event" container=14dcd9a111650ae1c79fd40098036d50eb11e8eceb6364554fe7237126b1b68d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:32:56 minikube dockerd[810]: time="2022-12-03T09:32:56.422307516Z" level=info msg="ignoring event" container=1672cd7a4a3afc38cbd6b399682e1124af1b1cd958ac5f299ed022ecd9b9bb6e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:05 minikube dockerd[810]: time="2022-12-03T09:33:05.415067115Z" level=info msg="ignoring event" container=09e63f5f769d122ef998273893df30216a9ea6851b90078eebe4fc9ad9775cd8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:05 minikube dockerd[810]: time="2022-12-03T09:33:05.612048619Z" level=info msg="ignoring event" container=a05c55d7d38ddfae0f6cfda7ee2308dfbe892d20db8f46a3659fa226222f7f3c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:16 minikube dockerd[810]: time="2022-12-03T09:33:16.612967304Z" level=info msg="ignoring event" container=07cb5e06bd427ce55df95e26546aa2c2802db04dbb870340db5b726e3f236f2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:18 minikube dockerd[810]: time="2022-12-03T09:33:18.123080288Z" level=info msg="ignoring event" container=8657d1d528c9f7829bb535f84673f11d2b029733e5bbdc24ad21add23755d850 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:19 minikube dockerd[810]: time="2022-12-03T09:33:19.711911475Z" level=info msg="ignoring event" container=6d5388e40b54294fd48f6ec605e620e586ad0b98a66cfa36b4509000f9b4c025 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:25 minikube dockerd[810]: time="2022-12-03T09:33:25.912199459Z" level=info msg="ignoring event" container=bcb1608460691b3be1338796488ddad7bc738fec23ae6b18ae848253988bf011 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:27 minikube dockerd[810]: time="2022-12-03T09:33:27.622826274Z" level=info msg="ignoring event" container=a68a589cd42fa9a4add7d1aa61c78c717cd76a7b489eb1ee19b1a238751e4456 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:31 minikube dockerd[810]: time="2022-12-03T09:33:31.215080144Z" level=info msg="ignoring event" container=7de0182480a944a23336dd5dc0298245dc123776c69e0f37c825bc5f72cf50b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:37 minikube dockerd[810]: time="2022-12-03T09:33:37.731288340Z" level=info msg="ignoring event" container=89badd0e50e1e5677a69984b352e6751378787cc44dc753cad0c101bf19961ef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:38 minikube dockerd[810]: time="2022-12-03T09:33:38.421214268Z" level=info msg="ignoring event" container=59d1b1a4e5cc291d9d10af5b44ac46b0e5d7ab15e650f0c5cffda64b02633ee4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:54 minikube dockerd[810]: time="2022-12-03T09:33:54.023318486Z" level=info msg="ignoring event" container=1dd5bb087c16dc7a12def94042f6e7ba157874193c3f9a411d84a660508181bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:33:54 minikube dockerd[810]: time="2022-12-03T09:33:54.713822667Z" level=info msg="ignoring event" container=fdb082ccbc5c8378c88edf6d36573e55eced4cbda710814f54c7ce483d5ce4fa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:02 minikube dockerd[810]: time="2022-12-03T09:34:02.022081888Z" level=info msg="ignoring event" container=f1079c4026dfe2c1d34dffd6eeeaa50c9312794aaab63f806e11306db2eb0d48 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:07 minikube dockerd[810]: time="2022-12-03T09:34:07.014201869Z" level=info msg="ignoring event" container=f5286b8f4d3a1f337752c191fd4cce1e7660c5d94ce6331fb91b117dfeac9132 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:08 minikube dockerd[810]: time="2022-12-03T09:34:08.715860856Z" level=info msg="ignoring event" container=92f8ab3f29bb3addc3c4aaebedfca95a5722cf96d5e788f98f94fab11d6d5b9b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:22 minikube dockerd[810]: time="2022-12-03T09:34:22.315714968Z" level=info msg="ignoring event" container=1988d0d7a6b610355d81f1459b75e1056e5243bbd366779b2b0439a1f9d951f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:23 minikube dockerd[810]: time="2022-12-03T09:34:23.807353284Z" level=info msg="ignoring event" container=dc0bae5262bd8ade891f5a812d0fc76948e83b4e31865f72d47dd1364dd4334b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:39 minikube dockerd[810]: time="2022-12-03T09:34:39.520649996Z" level=info msg="ignoring event" container=33a7b5120618e7f64cb9e7a3da9d409977c117ac166b0ef431e14c1e37a67058 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:41 minikube dockerd[810]: time="2022-12-03T09:34:41.229405620Z" level=info msg="ignoring event" container=ba215f3dbbb3f753730e1433cf9576ac52a616e1df979df43ddf65a14ce614b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:58 minikube dockerd[810]: time="2022-12-03T09:34:58.829378378Z" level=info msg="ignoring event" container=6548070a16f7fa83490b1932ab4b2b7e77483a254ad31780dc5d2bef61b4463e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:34:59 minikube dockerd[810]: time="2022-12-03T09:34:59.429014400Z" level=info msg="ignoring event" container=75f5c12f736f5a7c3d4bde96a289dbdf78028fde7eeef96be2a931658f2affbe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:35:18 minikube dockerd[810]: time="2022-12-03T09:35:18.608835826Z" level=info msg="ignoring event" container=1d48f5eb9ba7c96646b65bd122b52868c4378fe338007fb4e986846e1e66769f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:35:19 minikube dockerd[810]: time="2022-12-03T09:35:19.208530217Z" level=info msg="ignoring event" container=89bc57b9f8f71557d4f95e8e57a1f4df0a02585dcdf19d71d0a66d3ce536cc06 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:35:36 minikube dockerd[810]: time="2022-12-03T09:35:36.712578722Z" level=info msg="ignoring event" container=4838539445b5e504b59942d0db773800544d68b3d3b3b279ae8dd91efb7993a1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 03 09:35:38 minikube dockerd[810]: time="2022-12-03T09:35:38.013401318Z" level=info msg="ignoring event" container=0393f9b67d51906222f0ba3b9b3913adb7fab35ef450732d3c7c5adeda94d5ed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                               CREATED             STATE               NAME                      ATTEMPT             POD ID
13f314b7ed4ff       calico/kube-controllers@sha256:a850ce8daa84433a5641900693b0f8bc8e5177a4aa4cac8cf4b6cd8c24fd9886     5 minutes ago       Running             calico-kube-controllers   0                   5bb3df17d8c8a
9ddc9fc301010       a4ca41631cc7a                                                                                       6 minutes ago       Running             coredns                   0                   8a116c3be16b0
3f3cc058f78e7       calico/node@sha256:7f9aa7e31fbcea7be64b153f8bcfd494de023679ec10d851a05667f0adb42650                 7 minutes ago       Running             calico-node               0                   8aadf09a41f0a
f1079c4026dfe       calico/pod2daemon-flexvol@sha256:c17e3e9871682bed00bfd33f8d6f00db1d1a126034a25bf5380355978e0c548d   8 minutes ago       Exited              flexvol-driver            0                   8aadf09a41f0a
7de0182480a94       4945b742b8e66                                                                                       9 minutes ago       Exited              install-cni               0                   8aadf09a41f0a
6d5388e40b542       calico/cni@sha256:9906e2cca8006e1fe9fc3f358a3a06da6253afdd6fad05d594e884e8298ffe1d                  9 minutes ago       Exited              upgrade-ipam              0                   8aadf09a41f0a
f34dbd624e07c       6e38f40d628db                                                                                       10 minutes ago      Running             storage-provisioner       0                   6eb514b08db24
d2a56f84514f3       77b49675beae1                                                                                       10 minutes ago      Running             kube-proxy                0                   23c3ef3ec03d6
9e846411c9100       e3ed7dee73e93                                                                                       11 minutes ago      Running             kube-scheduler            0                   15b77173ec764
3b7f499455c54       88784fb4ac2f6                                                                                       11 minutes ago      Running             kube-controller-manager   0                   ece1ea20b38fc
6ae030971ae15       aebe758cef4cd                                                                                       11 minutes ago      Running             etcd                      0                   1ffb30a3efbf0
9a708b8f69f03       529072250ccc6                                                                                       11 minutes ago      Running             kube-apiserver            0                   3e55264ef6054

* 
* ==> coredns [9ddc9fc30101] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=4243041b7a72319b9be7842a7d34b6767bbdac2b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_03T12_32_16_0700
                    minikube.k8s.io/version=v1.27.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 192.168.49.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.120.64
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 03 Dec 2022 09:31:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 03 Dec 2022 09:43:17 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sat, 03 Dec 2022 09:35:42 +0000   Sat, 03 Dec 2022 09:35:42 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Sat, 03 Dec 2022 09:42:25 +0000   Sat, 03 Dec 2022 09:31:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Sat, 03 Dec 2022 09:42:25 +0000   Sat, 03 Dec 2022 09:31:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Sat, 03 Dec 2022 09:42:25 +0000   Sat, 03 Dec 2022 09:31:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Sat, 03 Dec 2022 09:42:25 +0000   Sat, 03 Dec 2022 09:32:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11316916Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11316916Ki
  pods:               110
System Info:
  Machine ID:                 40026c506a9948afaae3b0fda0a61c83
  System UUID:                40026c506a9948afaae3b0fda0a61c83
  Boot ID:                    5c05681d-0dd7-4181-ae72-3b4c5e77dc6c
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.0
  Kube-Proxy Version:         v1.24.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                      ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-c44b4545-879dj    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 calico-node-jjx6b                         250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 coredns-6d4b75cb6d-sgpn2                  100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     10m
  kube-system                 etcd-minikube                             100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         11m
  kube-system                 kube-apiserver-minikube                   250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-controller-manager-minikube          200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 kube-proxy-sg5jc                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 kube-scheduler-minikube                   100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  kube-system                 storage-provisioner                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                1 (8%!)(MISSING)      0 (0%!)(MISSING)
  memory             170Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 10m                kube-proxy       
  Normal  NodeHasSufficientMemory  11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  11m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 11m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                11m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Dec 3 08:23] WSL2: Performing memory compaction.
[Dec 3 08:24] WSL2: Performing memory compaction.
[Dec 3 08:25] WSL2: Performing memory compaction.
[Dec 3 08:26] WSL2: Performing memory compaction.
[Dec 3 08:27] WSL2: Performing memory compaction.
[Dec 3 08:28] WSL2: Performing memory compaction.
[Dec 3 08:29] WSL2: Performing memory compaction.
[Dec 3 08:30] WSL2: Performing memory compaction.
[Dec 3 08:31] WSL2: Performing memory compaction.
[Dec 3 08:32] WSL2: Performing memory compaction.
[Dec 3 08:33] WSL2: Performing memory compaction.
[Dec 3 08:34] WSL2: Performing memory compaction.
[Dec 3 08:35] WSL2: Performing memory compaction.
[Dec 3 08:37] WSL2: Performing memory compaction.
[Dec 3 08:38] WSL2: Performing memory compaction.
[Dec 3 08:39] WSL2: Performing memory compaction.
[Dec 3 08:40] WSL2: Performing memory compaction.
[Dec 3 08:41] WSL2: Performing memory compaction.
[Dec 3 08:42] WSL2: Performing memory compaction.
[Dec 3 08:43] WSL2: Performing memory compaction.
[Dec 3 08:44] WSL2: Performing memory compaction.
[Dec 3 08:45] WSL2: Performing memory compaction.
[Dec 3 08:46] WSL2: Performing memory compaction.
[Dec 3 08:47] WSL2: Performing memory compaction.
[Dec 3 08:48] WSL2: Performing memory compaction.
[Dec 3 08:49] WSL2: Performing memory compaction.
[Dec 3 08:50] WSL2: Performing memory compaction.
[Dec 3 08:51] WSL2: Performing memory compaction.
[Dec 3 08:52] WSL2: Performing memory compaction.
[Dec 3 08:53] WSL2: Performing memory compaction.
[Dec 3 08:54] WSL2: Performing memory compaction.
[Dec 3 08:55] WSL2: Performing memory compaction.
[Dec 3 08:56] WSL2: Performing memory compaction.
[Dec 3 08:57] WSL2: Performing memory compaction.
[Dec 3 08:58] WSL2: Performing memory compaction.
[Dec 3 08:59] WSL2: Performing memory compaction.
[Dec 3 09:00] WSL2: Performing memory compaction.
[Dec 3 09:01] WSL2: Performing memory compaction.
[Dec 3 09:02] WSL2: Performing memory compaction.
[Dec 3 09:03] WSL2: Performing memory compaction.
[Dec 3 09:04] WSL2: Performing memory compaction.
[Dec 3 09:05] WSL2: Performing memory compaction.
[Dec 3 09:06] WSL2: Performing memory compaction.
[Dec 3 09:07] WSL2: Performing memory compaction.
[Dec 3 09:08] WSL2: Performing memory compaction.
[Dec 3 09:09] WSL2: Performing memory compaction.
[Dec 3 09:10] WSL2: Performing memory compaction.
[Dec 3 09:11] WSL2: Performing memory compaction.
[Dec 3 09:12] WSL2: Performing memory compaction.
[Dec 3 09:14] WSL2: Performing memory compaction.
[Dec 3 09:17] WSL2: Performing memory compaction.
[Dec 3 09:23] WSL2: Performing memory compaction.
[Dec 3 09:24] WSL2: Performing memory compaction.
[Dec 3 09:25] WSL2: Performing memory compaction.
[Dec 3 09:26] WSL2: Performing memory compaction.
[Dec 3 09:27] WSL2: Performing memory compaction.
[Dec 3 09:28] WSL2: Performing memory compaction.
[Dec 3 09:29] WSL2: Performing memory compaction.
[Dec 3 09:30] WSL2: Performing memory compaction.
[Dec 3 09:31] WSL2: Performing memory compaction.

* 
* ==> etcd [6ae030971ae1] <==
* {"level":"info","ts":"2022-12-03T09:43:18.913Z","caller":"traceutil/trace.go:171","msg":"trace[410686441] transaction","detail":"{read_only:false; response_revision:927; number_of_response:1; }","duration":"700.116976ms","start":"2022-12-03T09:43:18.213Z","end":"2022-12-03T09:43:18.913Z","steps":["trace[410686441] 'compare'  (duration: 609.616971ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:18.914Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:18.213Z","time spent":"700.273449ms","remote":"127.0.0.1:40292","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":118,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:923 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128017487856599964 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-12-03T09:43:19.323Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.61396ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:422"}
{"level":"info","ts":"2022-12-03T09:43:19.323Z","caller":"traceutil/trace.go:171","msg":"trace[1132733775] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:928; }","duration":"109.726651ms","start":"2022-12-03T09:43:19.214Z","end":"2022-12-03T09:43:19.323Z","steps":["trace[1132733775] 'range keys from in-memory index tree'  (duration: 109.465788ms)"],"step_count":1}
{"level":"info","ts":"2022-12-03T09:43:19.713Z","caller":"traceutil/trace.go:171","msg":"trace[1170096366] linearizableReadLoop","detail":"{readStateIndex:1080; appliedIndex:1080; }","duration":"299.352483ms","start":"2022-12-03T09:43:19.414Z","end":"2022-12-03T09:43:19.713Z","steps":["trace[1170096366] 'read index received'  (duration: 299.342374ms)","trace[1170096366] 'applied index is now lower than readState.Index'  (duration: 7.965¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:19.713Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.066714ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2022-12-03T09:43:19.714Z","caller":"traceutil/trace.go:171","msg":"trace[13572093] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:929; }","duration":"100.113741ms","start":"2022-12-03T09:43:19.613Z","end":"2022-12-03T09:43:19.714Z","steps":["trace[13572093] 'agreement among raft nodes before linearized reading'  (duration: 100.013573ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:19.714Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"299.99635ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-node-lease/minikube\" ","response":"range_response_count:1 size:538"}
{"level":"info","ts":"2022-12-03T09:43:19.714Z","caller":"traceutil/trace.go:171","msg":"trace[632140266] range","detail":"{range_begin:/registry/leases/kube-node-lease/minikube; range_end:; response_count:1; response_revision:929; }","duration":"300.020074ms","start":"2022-12-03T09:43:19.414Z","end":"2022-12-03T09:43:19.714Z","steps":["trace[632140266] 'agreement among raft nodes before linearized reading'  (duration: 299.965261ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:19.723Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:19.414Z","time spent":"309.086405ms","remote":"127.0.0.1:40354","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":1,"response size":562,"request content":"key:\"/registry/leases/kube-node-lease/minikube\" "}
{"level":"warn","ts":"2022-12-03T09:43:20.113Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"299.34504ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/bgpconfigurations/\" range_end:\"/registry/crd.projectcalico.org/bgpconfigurations0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:20.113Z","caller":"traceutil/trace.go:171","msg":"trace[1973649223] range","detail":"{range_begin:/registry/crd.projectcalico.org/bgpconfigurations/; range_end:/registry/crd.projectcalico.org/bgpconfigurations0; response_count:0; response_revision:929; }","duration":"299.455266ms","start":"2022-12-03T09:43:19.814Z","end":"2022-12-03T09:43:20.113Z","steps":["trace[1973649223] 'agreement among raft nodes before linearized reading'  (duration: 299.300607ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:20.113Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"299.708561ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:20.114Z","caller":"traceutil/trace.go:171","msg":"trace[154528934] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:929; }","duration":"299.770537ms","start":"2022-12-03T09:43:19.814Z","end":"2022-12-03T09:43:20.114Z","steps":["trace[154528934] 'agreement among raft nodes before linearized reading'  (duration: 299.70241ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:21.223Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.354279ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/clusterinformations/default\" ","response":"range_response_count:1 size:920"}
{"level":"info","ts":"2022-12-03T09:43:21.223Z","caller":"traceutil/trace.go:171","msg":"trace[904462215] range","detail":"{range_begin:/registry/crd.projectcalico.org/clusterinformations/default; range_end:; response_count:1; response_revision:929; }","duration":"109.464616ms","start":"2022-12-03T09:43:21.114Z","end":"2022-12-03T09:43:21.223Z","steps":["trace[904462215] 'range keys from in-memory index tree'  (duration: 109.196023ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:23.913Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"399.9918ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:23.914Z","caller":"traceutil/trace.go:171","msg":"trace[691240714] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:929; }","duration":"400.130349ms","start":"2022-12-03T09:43:23.513Z","end":"2022-12-03T09:43:23.914Z","steps":["trace[691240714] 'range keys from in-memory index tree'  (duration: 399.803176ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:23.914Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:23.513Z","time spent":"400.202254ms","remote":"127.0.0.1:40308","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-03T09:43:23.923Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.689946ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128017487856599989 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:928 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2022-12-03T09:43:23.923Z","caller":"traceutil/trace.go:171","msg":"trace[1033616670] transaction","detail":"{read_only:false; response_revision:930; number_of_response:1; }","duration":"199.218279ms","start":"2022-12-03T09:43:23.724Z","end":"2022-12-03T09:43:23.923Z","steps":["trace[1033616670] 'process raft request'  (duration: 89.45572ms)","trace[1033616670] 'compare'  (duration: 100.112474ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:28.613Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"300.068996ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128017487856600007 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:927 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128017487856600003 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2022-12-03T09:43:28.623Z","caller":"traceutil/trace.go:171","msg":"trace[212866631] transaction","detail":"{read_only:false; response_revision:931; number_of_response:1; }","duration":"389.87169ms","start":"2022-12-03T09:43:28.233Z","end":"2022-12-03T09:43:28.623Z","steps":["trace[212866631] 'process raft request'  (duration: 79.548358ms)","trace[212866631] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:116; } (duration: 299.899068ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:28.624Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:28.233Z","time spent":"389.990493ms","remote":"127.0.0.1:40292","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":118,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:927 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128017487856600003 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-12-03T09:43:29.113Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"399.777604ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128017487856600009 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:930 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2022-12-03T09:43:29.113Z","caller":"traceutil/trace.go:171","msg":"trace[269499577] linearizableReadLoop","detail":"{readStateIndex:1086; appliedIndex:1085; }","duration":"290.415009ms","start":"2022-12-03T09:43:28.823Z","end":"2022-12-03T09:43:29.113Z","steps":["trace[269499577] 'read index received'  (duration: 116.919¬µs)","trace[269499577] 'applied index is now lower than readState.Index'  (duration: 290.297038ms)"],"step_count":2}
{"level":"info","ts":"2022-12-03T09:43:29.113Z","caller":"traceutil/trace.go:171","msg":"trace[398713787] transaction","detail":"{read_only:false; response_revision:933; number_of_response:1; }","duration":"589.811531ms","start":"2022-12-03T09:43:28.524Z","end":"2022-12-03T09:43:29.113Z","steps":["trace[398713787] 'process raft request'  (duration: 189.761647ms)","trace[398713787] 'compare'  (duration: 399.632191ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:29.113Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:28.524Z","time spent":"589.842037ms","remote":"127.0.0.1:40318","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1095,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:930 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2022-12-03T09:43:29.114Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"290.766207ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/bgppeers/\" range_end:\"/registry/crd.projectcalico.org/bgppeers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:29.114Z","caller":"traceutil/trace.go:171","msg":"trace[274948709] range","detail":"{range_begin:/registry/crd.projectcalico.org/bgppeers/; range_end:/registry/crd.projectcalico.org/bgppeers0; response_count:0; response_revision:933; }","duration":"290.794741ms","start":"2022-12-03T09:43:28.823Z","end":"2022-12-03T09:43:29.114Z","steps":["trace[274948709] 'agreement among raft nodes before linearized reading'  (duration: 290.743565ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:29.114Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"200.673029ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-12-03T09:43:29.114Z","caller":"traceutil/trace.go:171","msg":"trace[1729682892] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:933; }","duration":"200.693738ms","start":"2022-12-03T09:43:28.913Z","end":"2022-12-03T09:43:29.114Z","steps":["trace[1729682892] 'agreement among raft nodes before linearized reading'  (duration: 200.653402ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:29.213Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"289.548791ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:422"}
{"level":"info","ts":"2022-12-03T09:43:29.213Z","caller":"traceutil/trace.go:171","msg":"trace[1731427133] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:933; }","duration":"289.608814ms","start":"2022-12-03T09:43:28.924Z","end":"2022-12-03T09:43:29.213Z","steps":["trace[1731427133] 'agreement among raft nodes before linearized reading'  (duration: 289.481315ms)"],"step_count":1}
{"level":"info","ts":"2022-12-03T09:43:32.013Z","caller":"traceutil/trace.go:171","msg":"trace[669841816] linearizableReadLoop","detail":"{readStateIndex:1087; appliedIndex:1087; }","duration":"199.253078ms","start":"2022-12-03T09:43:31.814Z","end":"2022-12-03T09:43:32.013Z","steps":["trace[669841816] 'read index received'  (duration: 199.233081ms)","trace[669841816] 'applied index is now lower than readState.Index'  (duration: 11.081¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:32.013Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"199.415803ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:32.013Z","caller":"traceutil/trace.go:171","msg":"trace[1580417543] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:934; }","duration":"199.475365ms","start":"2022-12-03T09:43:31.814Z","end":"2022-12-03T09:43:32.013Z","steps":["trace[1580417543] 'agreement among raft nodes before linearized reading'  (duration: 199.373434ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:33.833Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"219.18437ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/crd.projectcalico.org/ipamhandles/\" range_end:\"/registry/crd.projectcalico.org/ipamhandles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-12-03T09:43:33.833Z","caller":"traceutil/trace.go:171","msg":"trace[741130571] range","detail":"{range_begin:/registry/crd.projectcalico.org/ipamhandles/; range_end:/registry/crd.projectcalico.org/ipamhandles0; response_count:0; response_revision:934; }","duration":"219.295508ms","start":"2022-12-03T09:43:33.614Z","end":"2022-12-03T09:43:33.833Z","steps":["trace[741130571] 'count revisions from in-memory index tree'  (duration: 209.677964ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:33.833Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"220.080218ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-12-03T09:43:33.833Z","caller":"traceutil/trace.go:171","msg":"trace[1398128425] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:934; }","duration":"220.10274ms","start":"2022-12-03T09:43:33.613Z","end":"2022-12-03T09:43:33.833Z","steps":["trace[1398128425] 'count revisions from in-memory index tree'  (duration: 219.87336ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:33.833Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"219.825551ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-12-03T09:43:33.833Z","caller":"traceutil/trace.go:171","msg":"trace[1031173428] range","detail":"{range_begin:/registry/leases/; range_end:/registry/leases0; response_count:0; response_revision:934; }","duration":"219.837183ms","start":"2022-12-03T09:43:33.614Z","end":"2022-12-03T09:43:33.833Z","steps":["trace[1031173428] 'count revisions from in-memory index tree'  (duration: 219.720975ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:35.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"299.397077ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128017487856600033 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:933 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2022-12-03T09:43:35.218Z","caller":"traceutil/trace.go:171","msg":"trace[277185473] linearizableReadLoop","detail":"{readStateIndex:1089; appliedIndex:1088; }","duration":"200.233025ms","start":"2022-12-03T09:43:35.018Z","end":"2022-12-03T09:43:35.218Z","steps":["trace[277185473] 'read index received'  (duration: 199.760149ms)","trace[277185473] 'applied index is now lower than readState.Index'  (duration: 471.343¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:35.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"200.363068ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:35.218Z","caller":"traceutil/trace.go:171","msg":"trace[1960886783] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:935; }","duration":"200.388245ms","start":"2022-12-03T09:43:35.018Z","end":"2022-12-03T09:43:35.218Z","steps":["trace[1960886783] 'agreement among raft nodes before linearized reading'  (duration: 200.306482ms)"],"step_count":1}
{"level":"info","ts":"2022-12-03T09:43:35.228Z","caller":"traceutil/trace.go:171","msg":"trace[467881782] transaction","detail":"{read_only:false; response_revision:935; number_of_response:1; }","duration":"309.571223ms","start":"2022-12-03T09:43:34.918Z","end":"2022-12-03T09:43:35.228Z","steps":["trace[467881782] 'compare'  (duration: 299.187935ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:35.228Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:34.918Z","time spent":"309.689314ms","remote":"127.0.0.1:40318","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1095,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:933 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2022-12-03T09:43:35.518Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"199.490247ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2022-12-03T09:43:35.518Z","caller":"traceutil/trace.go:171","msg":"trace[1116078922] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:935; }","duration":"199.610614ms","start":"2022-12-03T09:43:35.318Z","end":"2022-12-03T09:43:35.518Z","steps":["trace[1116078922] 'count revisions from in-memory index tree'  (duration: 199.295493ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:38.128Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:37.819Z","time spent":"309.155705ms","remote":"127.0.0.1:40292","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2022-12-03T09:43:38.618Z","caller":"traceutil/trace.go:171","msg":"trace[912025101] transaction","detail":"{read_only:false; response_revision:936; number_of_response:1; }","duration":"189.617642ms","start":"2022-12-03T09:43:38.428Z","end":"2022-12-03T09:43:38.618Z","steps":["trace[912025101] 'process raft request'  (duration: 89.711399ms)","trace[912025101] 'compare'  (duration: 99.727158ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-03T09:43:39.228Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.709144ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2022-12-03T09:43:39.228Z","caller":"traceutil/trace.go:171","msg":"trace[1805608208] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:936; }","duration":"109.8087ms","start":"2022-12-03T09:43:39.118Z","end":"2022-12-03T09:43:39.228Z","steps":["trace[1805608208] 'range keys from in-memory index tree'  (duration: 109.550477ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:39.228Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"299.990168ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2022-12-03T09:43:39.228Z","caller":"traceutil/trace.go:171","msg":"trace[1760894142] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:936; }","duration":"300.007811ms","start":"2022-12-03T09:43:38.928Z","end":"2022-12-03T09:43:39.228Z","steps":["trace[1760894142] 'count revisions from in-memory index tree'  (duration: 299.875513ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-03T09:43:39.228Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-03T09:43:38.928Z","time spent":"300.035963ms","remote":"127.0.0.1:40384","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":2,"response size":31,"request content":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true "}
{"level":"warn","ts":"2022-12-03T09:43:39.228Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"209.995966ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:422"}
{"level":"info","ts":"2022-12-03T09:43:39.228Z","caller":"traceutil/trace.go:171","msg":"trace[1909207191] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:936; }","duration":"210.016173ms","start":"2022-12-03T09:43:39.018Z","end":"2022-12-03T09:43:39.228Z","steps":["trace[1909207191] 'range keys from in-memory index tree'  (duration: 209.900366ms)"],"step_count":1}

* 
* ==> kernel <==
*  09:43:42 up 21:10,  0 users,  load average: 13.26, 9.66, 6.65
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [9a708b8f69f0] <==
* I1203 09:43:29.114038       1 trace.go:205] Trace[2036950111]: "Patch" url:/api/v1/namespaces/kube-system/events/calico-node-jjx6b.172d3f28151f67eb,user-agent:kubelet/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:be9c5c20-e6ec-41bd-9089-97f6950ab67b,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Dec-2022 09:43:28.113) (total time: 1000ms):
Trace[2036950111]: ---"About to apply patch" 200ms (09:43:28.313)
Trace[2036950111]: ---"Object stored in database" 799ms (09:43:29.113)
Trace[2036950111]: [1.000518811s] [1.000518811s] END
I1203 09:43:29.323713       1 trace.go:205] Trace[1625490939]: "GuaranteedUpdate etcd3" type:*core.Endpoints (03-Dec-2022 09:43:28.214) (total time: 1109ms):
Trace[1625490939]: ---"Transaction committed" 1009ms (09:43:29.323)
Trace[1625490939]: [1.109296889s] [1.109296889s] END
I1203 09:43:29.323767       1 trace.go:205] Trace[600802609]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:d37de864-251e-479c-b3d8-d18a2be504b2,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:28.814) (total time: 509ms):
Trace[600802609]: ---"About to write a response" 509ms (09:43:29.323)
Trace[600802609]: [509.354525ms] [509.354525ms] END
I1203 09:43:29.323857       1 trace.go:205] Trace[1895631308]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:b6d39f2c-3ca3-4a17-803c-81b054667ee8,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:28.214) (total time: 1109ms):
Trace[1895631308]: ---"Object stored in database" 1109ms (09:43:29.323)
Trace[1895631308]: [1.109721866s] [1.109721866s] END
I1203 09:43:32.013741       1 trace.go:205] Trace[2048439031]: "GuaranteedUpdate etcd3" type:*coordination.Lease (03-Dec-2022 09:43:31.323) (total time: 600ms):
Trace[2048439031]: ---"Transaction committed" 510ms (09:43:31.924)
Trace[2048439031]: [600.370963ms] [600.370963ms] END
I1203 09:43:32.014087       1 trace.go:205] Trace[649558613]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:312fc67f-1d20-44a0-b9ac-93dc229a963e,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Dec-2022 09:43:31.323) (total time: 690ms):
Trace[649558613]: ---"Object stored in database" 689ms (09:43:32.013)
Trace[649558613]: [690.218211ms] [690.218211ms] END
I1203 09:43:35.518565       1 trace.go:205] Trace[435291616]: "GuaranteedUpdate etcd3" type:*core.Endpoints (03-Dec-2022 09:43:34.828) (total time: 690ms):
Trace[435291616]: ---"Transaction committed" 689ms (09:43:35.518)
Trace[435291616]: [690.13464ms] [690.13464ms] END
I1203 09:43:35.518841       1 trace.go:205] Trace[592852574]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:fe6c33ea-bb51-4885-9df2-69e6e782594f,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:34.718) (total time: 799ms):
Trace[592852574]: ---"About to convert to expected version" 109ms (09:43:34.828)
Trace[592852574]: ---"Object stored in database" 690ms (09:43:35.518)
Trace[592852574]: [799.812239ms] [799.812239ms] END
I1203 09:43:38.828526       1 trace.go:205] Trace[71329011]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (03-Dec-2022 09:43:37.618) (total time: 1210ms):
Trace[71329011]: ---"initial value restored" 199ms (09:43:37.818)
Trace[71329011]: ---"Transaction prepared" 499ms (09:43:38.318)
Trace[71329011]: ---"Transaction committed" 510ms (09:43:38.828)
Trace[71329011]: [1.210101051s] [1.210101051s] END
I1203 09:43:44.018376       1 trace.go:205] Trace[313463475]: "GuaranteedUpdate etcd3" type:*coordination.Lease (03-Dec-2022 09:43:43.428) (total time: 589ms):
Trace[313463475]: ---"Transaction prepared" 289ms (09:43:43.718)
Trace[313463475]: ---"Transaction committed" 300ms (09:43:44.018)
Trace[313463475]: [589.385698ms] [589.385698ms] END
I1203 09:43:44.018799       1 trace.go:205] Trace[1247285329]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:2410a6b0-4d18-4df9-9188-bd5608e35760,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Dec-2022 09:43:43.428) (total time: 589ms):
Trace[1247285329]: ---"Object stored in database" 589ms (09:43:44.018)
Trace[1247285329]: [589.83543ms] [589.83543ms] END
I1203 09:43:47.528844       1 trace.go:205] Trace[1673387987]: "GuaranteedUpdate etcd3" type:*core.Endpoints (03-Dec-2022 09:43:46.928) (total time: 600ms):
Trace[1673387987]: ---"Transaction committed" 499ms (09:43:47.528)
Trace[1673387987]: [600.032534ms] [600.032534ms] END
I1203 09:43:47.618207       1 trace.go:205] Trace[1325710267]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:805b5bd2-fd27-43ea-b3fe-51b23dff16bc,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:46.928) (total time: 689ms):
Trace[1325710267]: ---"Object stored in database" 600ms (09:43:47.528)
Trace[1325710267]: ---"Writing http response done" 89ms (09:43:47.618)
Trace[1325710267]: [689.72132ms] [689.72132ms] END
I1203 09:43:48.028476       1 trace.go:205] Trace[1956703288]: "Get" url:/api/v1/namespaces/default/services/kubernetes,user-agent:kube-apiserver/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:41e2b450-85b5-4ab4-bda0-754b4eca0792,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:47.418) (total time: 610ms):
Trace[1956703288]: ---"About to write a response" 610ms (09:43:48.028)
Trace[1956703288]: [610.287201ms] [610.287201ms] END
I1203 09:43:49.928609       1 trace.go:205] Trace[1009104814]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (03-Dec-2022 09:43:48.428) (total time: 1500ms):
Trace[1009104814]: ---"initial value restored" 200ms (09:43:48.628)
Trace[1009104814]: ---"Transaction prepared" 489ms (09:43:49.118)
Trace[1009104814]: ---"Transaction committed" 810ms (09:43:49.928)
Trace[1009104814]: [1.500351747s] [1.500351747s] END
I1203 09:43:50.618625       1 trace.go:205] Trace[145788437]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.24.0 (linux/amd64) kubernetes/4ce5a89,audit-id:aceb7f32-8789-4426-953a-f4e0a07dea2f,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:50.018) (total time: 599ms):
Trace[145788437]: ---"About to write a response" 599ms (09:43:50.618)
Trace[145788437]: [599.72879ms] [599.72879ms] END
I1203 09:43:55.318529       1 trace.go:205] Trace[1816706036]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:6ee6119e-5c02-4692-96f1-a4a98b1d0454,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Dec-2022 09:43:54.718) (total time: 599ms):
Trace[1816706036]: ---"Conversion done" 209ms (09:43:54.928)
Trace[1816706036]: ---"Object stored in database" 389ms (09:43:55.318)
Trace[1816706036]: [599.592085ms] [599.592085ms] END

* 
* ==> kube-controller-manager [3b7f499455c5] <==
* E1203 09:33:29.324761       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.324802       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.324845       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.332044       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.332096       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.332138       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.414128       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.414166       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.414203       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.422250       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.422284       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.422357       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.512036       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.512066       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.512110       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.512963       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.512976       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.512993       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.514010       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.514025       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.514045       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:29.522114       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:29.522162       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:29.522239       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.114535       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.120949       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.121008       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.121361       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.121378       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.121395       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.121939       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.121955       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.121988       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.122230       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.122251       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.122270       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.123298       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.123323       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.123336       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.123569       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.123606       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.123623       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.123944       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.123959       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.123975       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.124199       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.124226       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.124243       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.124503       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.124515       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.124531       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.211496       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.211522       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.211587       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.212206       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.212233       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.212254       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"
E1203 09:33:36.212471       1 driver-call.go:262] Failed to unmarshal output for command: init, output: "", error: unexpected end of JSON input
W1203 09:33:36.212484       1 driver-call.go:149] FlexVolume: driver call failed: executable: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds, args: [init], error: fork/exec /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds/uds: no such file or directory, output: ""
E1203 09:33:36.212501       1 plugins.go:740] "Error dynamically probing plugins" err="error creating Flexvolume plugin from directory nodeagent~uds, skipping. Error: unexpected end of JSON input"

* 
* ==> kube-proxy [d2a56f84514f] <==
* E1203 09:32:30.023282       1 proxier.go:657] "Failed to read builtin modules file, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.10.16.3-microsoft-standard-WSL2/modules.builtin"
I1203 09:32:30.111927       1 proxier.go:667] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I1203 09:32:30.121089       1 proxier.go:667] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I1203 09:32:30.122689       1 proxier.go:667] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I1203 09:32:30.124163       1 proxier.go:667] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I1203 09:32:30.231061       1 proxier.go:667] "Failed to load kernel module with modprobe, you can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I1203 09:32:30.514590       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1203 09:32:30.520975       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1203 09:32:30.521052       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1203 09:32:32.112154       1 server_others.go:206] "Using iptables Proxier"
I1203 09:32:32.112198       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1203 09:32:32.112208       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1203 09:32:32.112235       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1203 09:32:32.112263       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1203 09:32:32.112423       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1203 09:32:32.112628       1 server.go:661] "Version info" version="v1.24.0"
I1203 09:32:32.112644       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1203 09:32:32.211573       1 config.go:444] "Starting node config controller"
I1203 09:32:32.211591       1 shared_informer.go:255] Waiting for caches to sync for node config
I1203 09:32:32.212142       1 config.go:317] "Starting service config controller"
I1203 09:32:32.212151       1 shared_informer.go:255] Waiting for caches to sync for service config
I1203 09:32:32.212188       1 config.go:226] "Starting endpoint slice config controller"
I1203 09:32:32.212192       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1203 09:32:32.413274       1 shared_informer.go:262] Caches are synced for node config
I1203 09:32:32.414293       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1203 09:32:32.414332       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [9e846411c910] <==
* E1203 09:31:58.426669       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1203 09:31:58.426703       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1203 09:31:58.426714       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1203 09:31:59.326762       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1203 09:31:59.326849       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1203 09:31:59.416133       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1203 09:31:59.416172       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1203 09:31:59.416173       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1203 09:31:59.416155       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1203 09:31:59.416191       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1203 09:31:59.416193       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1203 09:31:59.516320       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1203 09:31:59.516360       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1203 09:31:59.516334       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1203 09:31:59.516404       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1203 09:31:59.616221       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1203 09:31:59.616266       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1203 09:31:59.618159       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1203 09:31:59.618198       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1203 09:31:59.736013       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1203 09:31:59.736047       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1203 09:31:59.747057       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1203 09:31:59.747119       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1203 09:31:59.915148       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1203 09:31:59.915213       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1203 09:31:59.916448       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1203 09:31:59.916485       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1203 09:31:59.916575       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1203 09:31:59.916587       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1203 09:31:59.916625       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1203 09:31:59.916633       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1203 09:32:00.077870       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1203 09:32:00.077925       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1203 09:32:01.347469       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1203 09:32:01.347531       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1203 09:32:01.536153       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1203 09:32:01.536210       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1203 09:32:01.717388       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1203 09:32:01.717422       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1203 09:32:01.816260       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1203 09:32:01.816305       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1203 09:32:01.916476       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1203 09:32:01.916527       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1203 09:32:02.117066       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1203 09:32:02.117096       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1203 09:32:02.316888       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1203 09:32:02.316938       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1203 09:32:02.516459       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1203 09:32:02.516505       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1203 09:32:02.516462       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1203 09:32:02.516538       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1203 09:32:02.617900       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1203 09:32:02.617934       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1203 09:32:02.617960       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1203 09:32:02.617968       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1203 09:32:02.627484       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1203 09:32:02.627518       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1203 09:32:02.775944       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1203 09:32:02.775977       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
I1203 09:32:08.421039       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Sat 2022-12-03 09:31:14 UTC, end at Sat 2022-12-03 09:44:43 UTC. --
Dec 03 09:40:52 minikube kubelet[2219]: E1203 09:40:52.719667    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:40:56 minikube kubelet[2219]: E1203 09:40:55.820585    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:40:57 minikube kubelet[2219]: E1203 09:40:57.109728    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:04 minikube kubelet[2219]: E1203 09:41:03.918783    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:04 minikube kubelet[2219]: E1203 09:41:04.820014    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:41:07 minikube kubelet[2219]: E1203 09:41:07.609350    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:08 minikube kubelet[2219]: E1203 09:41:08.308710    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:12 minikube kubelet[2219]: E1203 09:41:12.019816    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:41:17 minikube kubelet[2219]: E1203 09:41:17.119344    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:41:18 minikube kubelet[2219]: E1203 09:41:18.209887    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:18 minikube kubelet[2219]: E1203 09:41:18.808939    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:24 minikube kubelet[2219]: E1203 09:41:24.318886    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:41:28 minikube kubelet[2219]: E1203 09:41:28.019177    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:41:28 minikube kubelet[2219]: E1203 09:41:28.019596    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:29 minikube kubelet[2219]: E1203 09:41:29.809824    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:33 minikube kubelet[2219]: E1203 09:41:33.219890    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:34 minikube kubelet[2219]: E1203 09:41:34.718987    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:41:40 minikube kubelet[2219]: E1203 09:41:40.109802    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:42 minikube kubelet[2219]: E1203 09:41:42.219921    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:41:43 minikube kubelet[2219]: E1203 09:41:43.418905    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:49 minikube kubelet[2219]: E1203 09:41:49.009259    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:41:50 minikube kubelet[2219]: E1203 09:41:50.119349    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:53 minikube kubelet[2219]: E1203 09:41:53.208719    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:41:59 minikube kubelet[2219]: E1203 09:41:59.009749    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:41:59 minikube kubelet[2219]: E1203 09:41:59.109257    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:42:01 minikube kubelet[2219]: E1203 09:42:01.609304    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:42:07 minikube kubelet[2219]: E1203 09:42:07.224050    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:42:07 minikube kubelet[2219]: E1203 09:42:07.914457    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:42:10 minikube kubelet[2219]: W1203 09:42:10.514858    2219 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 03 09:42:16 minikube kubelet[2219]: E1203 09:42:15.914061    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:42:17 minikube kubelet[2219]: E1203 09:42:17.214203    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:42:23 minikube kubelet[2219]: E1203 09:42:23.324423    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:42:25 minikube kubelet[2219]: E1203 09:42:25.013928    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:42:33 minikube kubelet[2219]: E1203 09:42:33.014006    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:42:34 minikube kubelet[2219]: E1203 09:42:34.118628    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:42:34 minikube kubelet[2219]: E1203 09:42:34.428961    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:42:37 minikube kubelet[2219]: E1203 09:42:37.918606    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:42:47 minikube kubelet[2219]: E1203 09:42:47.318674    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:42:50 minikube kubelet[2219]: E1203 09:42:50.819329    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:42:51 minikube kubelet[2219]: E1203 09:42:51.219539    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:42:55 minikube kubelet[2219]: E1203 09:42:55.318915    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:43:05 minikube kubelet[2219]: E1203 09:43:05.814110    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:43:09 minikube kubelet[2219]: E1203 09:43:09.613797    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:43:09 minikube kubelet[2219]: E1203 09:43:09.613797    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:43:15 minikube kubelet[2219]: E1203 09:43:15.024050    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:43:24 minikube kubelet[2219]: E1203 09:43:24.223662    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:43:26 minikube kubelet[2219]: E1203 09:43:26.314332    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:43:26 minikube kubelet[2219]: E1203 09:43:26.323605    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:43:34 minikube kubelet[2219]: E1203 09:43:33.824074    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:43:43 minikube kubelet[2219]: E1203 09:43:43.628236    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:43:50 minikube kubelet[2219]: E1203 09:43:50.728614    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:43:50 minikube kubelet[2219]: E1203 09:43:50.728795    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:43:59 minikube kubelet[2219]: E1203 09:43:58.828870    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:44:07 minikube kubelet[2219]: E1203 09:44:07.914026    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]
Dec 03 09:44:12 minikube kubelet[2219]: I1203 09:44:12.324065    2219 trace.go:205] Trace[1433310324]: "iptables ChainExists" (03-Dec-2022 09:44:09.323) (total time: 3000ms):
Dec 03 09:44:12 minikube kubelet[2219]: Trace[1433310324]: [3.000046683s] [3.000046683s] END
Dec 03 09:44:17 minikube kubelet[2219]: E1203 09:44:17.224311    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-live -bird-live]
Dec 03 09:44:17 minikube kubelet[2219]: E1203 09:44:17.224476    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="3f3cc058f78e7762cc72a6029d87b4cafdc37f8a517235b970be8d6426f613ec" cmd=[/bin/calico-node -felix-ready -bird-ready]
Dec 03 09:44:32 minikube kubelet[2219]: E1203 09:44:32.323997    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -r]
Dec 03 09:44:40 minikube kubelet[2219]: E1203 09:44:40.309788    2219 remote_runtime.go:680] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = deadline exceeded (\"DeadlineExceeded\"): context deadline exceeded" containerID="13f314b7ed4ff54dc930af610a82d510c5280e6a6b8d93ce52b858b3c10239a2" cmd=[/usr/bin/check-status -l]

* 
* ==> storage-provisioner [f34dbd624e07] <==
